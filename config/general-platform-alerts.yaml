apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    prometheus: general-platform-alerts
    role: general-platform-alert-rules
    app: kube-prometheus-stack
    release: ${ helm_release }
  name: general-platform-alerts
  namespace: ${ helm_namespace }
spec:
  groups:
  - name: certificates.rules
    rules:
    - alert: SSLCertExpiringSoon
      expr: sum by (target, instance) (probe_ssl_earliest_cert_expiry{job="blackbox-exporter-prometheus-blackbox-exporter"} - time()) < 86400 * 20
      for: 2m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: 'SSL certificate for {{ $labels.instance }} expires in less than 20 days.'
  - name: job.rules
    rules:
    - alert: CompletedJobsNotCleared
      expr: count by (namespace) (kube_job_status_completion_time < (time() - 86400)) > 20
      for: 15m
      labels:
        severity: major
        scope: platform
      annotations:
        message: 'Namespace {{ $labels.namespace }} has {{ $value }} completed jobs older than 24h.'
  - name: meta-alerts.rules
    rules:
    - record: alerts_firing
      expr: sum without (alertname, alertstate) (label_replace(ALERTS{alertstate="firing",alertname!~"ManyAlertsFiring|ManyManyAlertsFiring|KubeJobCompletion|KubeJobFailed"}, "alertfiring", "$1", "alertname", "(.*)"))
    - alert: ManyManyAlertsFiring
      expr: sum by(alertfiring, namespace) (alerts_firing) > 50
      for: 2m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: '{{ $value }} instances of alert {{ $labels.alertfiring }} are firing in namespace {{ $labels.namespace }}!'
    - alert: ManyAlertsFiring
      expr: sum by(alertfiring, namespace) (alerts_firing) > 20
      for: 2m
      labels:
        severity: major
        scope: platform
      annotations:
        message: '{{ $value }} instances of alert {{ $labels.alertfiring }} are firing in namespace {{ $labels.namespace }}.'
  - name: nodepool-status.rules
    rules:
    - record: nodepool_allocatable_pods
      expr: sum(kube_node_status_allocatable_pods * on (node) group_left(label_agentpool) kube_node_labels) by (label_agentpool)
    - record: nodepool_allocated_pods
      expr: sum(kube_pod_info * on (node) group_left(label_agentpool) kube_node_labels) by (label_agentpool)
    - alert: NodepoolPodsFull
      expr: nodepool_allocated_pods/nodepool_allocatable_pods * 100 > 95
      for: 2m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: '{{ if eq $labels.label_agentpool ""}}Unpooled node{{ else }}Nodepool {{ $labels.label_agentpool }}{{end}} pod capacity is {{ printf "%.2f" $value }}% full!'
    - alert: NodepoolLowPodCapacity
      expr: nodepool_allocated_pods/nodepool_allocatable_pods * 100 > 85
      for: 10m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: '{{ if eq $labels.label_agentpool ""}}Unpooled node{{ else }}Nodepool {{ $labels.label_agentpool }}{{end}} pod capacity is {{ printf "%.2f" $value }}% full.'
  - name: node-status.rules
    rules:
    - record: node_cpu_available
      expr: sum by(nodename) (avg by(instance, job) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * on(instance, job) group_left(nodename) node_uname_info * 100)
    - alert: NodeDiskPressure
      expr: sum by (node) (kube_node_status_condition{condition="DiskPressure",job="kube-state-metrics",status="true"}) == 1
      for: 2m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: 'UNHEALTHY NODE: {{ $labels.node }} has critically low disk capacity.'
    - alert: NodeMemoryPressure
      expr: sum by (node) (kube_node_status_condition{condition="MemoryPressure",job="kube-state-metrics",status="true"}) == 1
      for: 2m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: 'UNHEALTHY NODE: {{ $labels.node }} has critically low memory.'
    - alert: NodePIDPressure
      expr: sum by (node) (kube_node_status_condition{condition="PIDPressure",job="kube-state-metrics",status="true"}) == 1
      for: 2m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: 'UNHEALTHY NODE: Too many processes are running on {{ $labels.node }}.'
    - alert: NodeNetworkUnavailable
      expr: sum by (node) (kube_node_status_condition{condition="NetworkUnavailable",job="kube-state-metrics",status="true"}) == 1
      for: 2m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: 'UNHEALTHY NODE: The network for {{ $labels.node }} is not correctly configured.'
    - alert: NodeNotReady
      expr: sum by (node) (kube_node_status_condition{condition="Ready",job="kube-state-metrics",status="true"}) == 0
      for: 2m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: '{{ $labels.node }} is not in a Ready state but did not trip a Network or Pressure condition.'
    - alert: NodeUnschedulable
      expr: sum by (node) (kube_node_spec_unschedulable{job="kube-state-metrics"}) == 1
      for: 1h
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: '{{ $labels.node }} is unschedulable for over 1 hour. If it is healthy, is it cordoned?'
    - alert: NodePodsFull
      expr: sum(kube_pod_info) by (node) / sum(kube_node_status_allocatable_pods) by (node) * 100 > 99
      for: 5m
      labels:
        severity: minor
        scope: platform
      annotations:
        message: '{{ $labels.node }} pod capacity is {{ printf "%.2f" $value }}% full!'
    - alert: NodeLowMemory
      expr: sum by (nodename) (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * on(instance,job) group_left(nodename) node_uname_info * 100) < 15
      for: 10m
      labels:
        severity: minor
        scope: platform
      annotations:
        message: '{{ $labels.nodename }} has {{ printf "%.2f" $value }}% available memory.'
    - alert: NodeLowDisk
      expr: sum by (nodename, device, mountpoint, fstype) (node_filesystem_avail_bytes / node_filesystem_size_bytes * on(instance,job) group_left(nodename) node_uname_info * 100) < 15
      for: 5m
      labels:
        severity: major
        scope: platform
      annotations:
        message: '{{ $labels.device }} on {{ $labels.nodename }} has {{ printf "%.2f" $value }}% available disk space.'
    - alert: NodeLowCPU
      expr: avg_over_time(node_cpu_available[3m:]) < 15
      for: 5m
      labels:
        severity: minor
        scope: platform
      annotations:
        message: '{{ $labels.nodename }} has {{ printf "%.2f" $value }}% available CPU.'
    - alert: NodeLowPodCapacity
      expr: sum(kube_pod_info) by (node) / sum(kube_node_status_allocatable_pods) by (node) * 100 > 90
      for: 5m
      labels:
        severity: minor
        scope: platform
      annotations:
        message: '{{ $labels.node }} pod capacity is {{ printf "%.2f" $value }}% full.'
  - name: prometheus-extended.rules
    rules:
    - alert: PrometheusStorageLow
      expr: >-
        sum by (persistentvolumeclaim, namespace, node) 
        (kubelet_volume_stats_available_bytes{persistentvolumeclaim="${ prometheus_pvc_name }"}
        /kubelet_volume_stats_capacity_bytes{persistentvolumeclaim="${ prometheus_pvc_name }"}) * 100 < 15
      for: 15m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: 'Prometheus storage has {{ printf "%.2f" $value }}% capacity remaining.'
  - name: velero.rules
    rules:
    - record: velero_schedule_failure_increment
      expr:  sum by(schedule) (velero_backup_failure_total{schedule!=""} - velero_backup_failure_total offset 10m)
    - record: velero_schedule_partial_failure_increment
      expr:  sum by(schedule) (velero_backup_partial_failure_total{schedule!=""} - velero_backup_partial_failure_total offset 10m)
    - alert: VeleroBackupFailure
      expr: velero_schedule_failure_increment > 0
      for: 15s
      labels:
        severity: urgent
        scope: platform
        resolves: never
      annotations:
        message: 'Failed backup in Velero schedule {{ $labels.schedule }}.'
    - alert: VeleroBackupPartialFailure
      expr: velero_schedule_partial_failure_increment > 0
      for: 15s
      labels:
        severity: urgent
        scope: platform
        resolves: never
      annotations:
        message: 'Partially failed backup in Velero schedule {{ $labels.schedule }}.'
    - alert: ContinuousVeleroBackupFailure
      expr: velero_schedule_failure_increment > 1
      for: 10m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: 'Continuous failed backup in Velero schedule {{ $labels.schedule }}!'
    - alert: ContinuousVeleroBackupPartialFailure
      expr: velero_schedule_partial_failure_increment > 1
      for: 10m
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: 'Continuous partially failed backup in Velero schedule {{ $labels.schedule }}!'
    - alert: VeleroHourlyBackupFailure
      expr: sum by(schedule) (velero_backup_failure_total{schedule="velero-hourly-resources"} - velero_backup_failure_total{schedule="velero-hourly-resources"} offset 65m > 0)
      for: 15s
      labels:
        severity: urgent
        scope: platform
      annotations:
        message: 'Hourly failure in backup schedule {{ $labels.schedule }}!'
    - alert: VeleroHourlyBackupPartialFailure
      expr: sum by(schedule) (velero_backup_partial_failure_total{schedule="velero-hourly-resources"} - velero_backup_partial_failure_total{schedule="velero-hourly-resources"} offset 65m > 0)
      for: 15s
      labels:
        severity: major
        scope: platform
      annotations:
        message: 'Hourly partial failure in backup schedule {{ $labels.schedule }}.'
